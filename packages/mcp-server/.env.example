# Quero Plant√£o MCP Server - Environment Variables
# Copy this file to .env and fill in your values

# =============================================================================
# REQUIRED: OpenAI API Configuration
# =============================================================================

# Your OpenAI API key (required for LLM-powered tools)
# Get one from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# =============================================================================
# OPTIONAL: LLM Configuration
# =============================================================================

# Model to use for analysis tools (default: gpt-4o-mini)
# Available models: gpt-4o-mini, gpt-4o, gpt-3.5-turbo
MCP_LLM_MODEL=gpt-4o-mini

# Temperature for LLM responses (0.0 = deterministic, 1.0 = creative)
# Lower values recommended for technical analysis (default: 0.1)
MCP_LLM_TEMPERATURE=0.1

# Maximum tokens for LLM responses (default: 4096)
# Increase for more detailed analysis, decrease for faster responses
MCP_LLM_MAX_TOKENS=4096

# =============================================================================
# OPTIONAL: Server Configuration
# =============================================================================

# Port for SSE mode (default: 8080)
# Only needed if running in HTTP mode
# MCP_SSE_PORT=8080

# Host for SSE mode (default: 127.0.0.1)
# Only needed if running in HTTP mode
# MCP_SSE_HOST=127.0.0.1

# =============================================================================
# DEVELOPMENT NOTES
# =============================================================================

# 1. Copy this file to .env: cp .env.example .env
# 2. Fill in your OPENAI_API_KEY
# 3. The MCP server will work without LLM tools if OPENAI_API_KEY is empty,
#    but business rules, code analysis, and context tools will be limited
# 4. For production use, consider using gpt-4o instead of gpt-4o-mini for better analysis